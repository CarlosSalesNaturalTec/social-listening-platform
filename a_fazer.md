* Gerar readme de cada módulo contendo : detalhes técnicos, instruções de operação e implementação, relação com os demais módulos do sistema.

* Gerar o contextDoc_analytics.md

    Assuma a persona de um especialista híbrido e multifacetado. Seu núcleo técnico é o de um:
    * Analista de Mídias Sociais com especialização em política.
    * Estrategista de Comunicação Digital.
    * Consultor de Marketing Político Digital.
    No entanto, sua expertise vai além. Você também possui as habilidades e a mentalidade de um:
    * Engenheiro de Software Sênior e Arquiteto de Nuvem.
    * Especialista em Python, Google Cloud Platform (GCP) e micro-serviços.

    Analise as informações fornecidas nos arquivos analytics.md, monitor_result.json e plano_de_acao.md .
    A partir delas você deve gerar um documento de contexto que servirá como base para construção do módulo de **ANALYTICS** da plataforma de social listening descrita no documento plano_de_acao_md. 
    O arquivo monitor_result.json possui a estrutura dos artigos analisados pela plataforma, utilizar exclusivamente estes dados para produzir as análises citadas em analytics.md.
    Gerar 

# EDITAR GEMINI.MD:
    * ao final de cada etapa gerar ou editar um arquivo readme.md contendo : detalhes técnicos, instruções de operação e implementação, relação com os demais módulos do sistema.
    * No Plano de ação. utilizar readme.md como referência/documentação dos módulos concluídos.
    * Ao criar funcionalidades, analisar arquivos readme.me e ver se não implica em nenhuma inconsistência entre os módulos.


# Diversos
* Scraper / coluna Snippet ... x horas atrás
* Analisar: Erro geral na tarefa de scraping : '_UnaryStreamMultiCallable' object has no attribute '_retry' mesmo quando alguns registros são processados com sucesso e outros com erro.
* Estratégia para:  scraper_failed, nlp_error, reprocess ? 

* google trends, dá para inserir ?
* instagram o que da pra fazer ?
* descritivo/explicativo abaixo de cada gráfico 

# Melhorias
* No Login, sempre dá erro na primeira vez, tem que tentar 2 vezes
* Acesso externo bloqueado para as APIs
* Analisar, remover e evitar deprecad

# Schedule
ok - 12:00hs e 20:00hs  - search
ok - 21:00hs            - historical_search
ok - 22:00hs            - scraper
ok - 23:00              - nlp



